# priors for occupancy model
beta0 ~ dnorm(0, 0.333)
betaTF ~ dnorm(0, 1)
betaYr14 ~ dnorm(0, 1)
betaYr15 ~ dnorm(0, 1)
betaYr16 ~ dnorm(0, 1)
betaYr17 ~ dnorm(0, 1)
betaYr18 ~ dnorm(0, 1)
betaYr19 ~ dnorm(0, 1)
betaPre ~ dnorm(0, 4) # Trt and Control sites should be similar pre-harvest
betaPost ~ dnorm(0, 0.25)
betaDW ~ dnorm(0, 0.25)
sd.b0 ~ dgamma(1, 2)
tau.b0 <- 1/(sd.b0 * sd.b0)
# treatment effect estimator
TrtEffect <- betaPost - betaPre
# priors for detection random effects
mu.a0 ~ dnorm(0, 0.333)
mu.a1 ~ dnorm(0, 1) # Trt
mu.a2 ~ dnorm(0, 1) # AT
mu.a3 ~ dnorm(0, 1) # AT^2
mu.a4 ~ dnorm(0, 1) # Trt*AT
mu.a5 ~ dnorm(0, 1) # Trt*AT^2
sd.a0 ~ dgamma(1, 2)
sd.a1 ~ dgamma(2, 1)
sd.a2 ~ dgamma(1, 2)
sd.a3 ~ dgamma(1, 2)
sd.a4 ~ dgamma(1, 2)
sd.a5 ~ dgamma(1, 2)
tau.a0 <- 1/(sd.a0 * sd.a0)
tau.a1 <- 1/(sd.a1 * sd.a1)
tau.a2 <- 1/(sd.a2 * sd.a2)
tau.a3 <- 1/(sd.a3 * sd.a3)
tau.a4 <- 1/(sd.a4 * sd.a4)
tau.a5 <- 1/(sd.a5 * sd.a5)
# detection model random effects (by year)
for(i in 1:nyear){
a0[i] ~ dnorm(mu.a0, tau.a0)
aTrt[i] ~ dnorm(mu.a1, tau.a1)
aAT[i] ~ dnorm(mu.a2, tau.a2)
aAT2[i] ~ dnorm(mu.a3, tau.a3)
aTrtAT[i] ~ dnorm(mu.a4, tau.a4)
aTrtAT2[i] ~ dnorm(mu.a5, tau.a5)
}
for(i in 1:nstand){
# occupancy stand-level effects
mu1[i] <- beta0 + betaTF*TFCL1[i]
mu1i[i] ~ dnorm(mu1[i], tau.b0)
b0[i] <- mu1i[i] - mu1[i] # random effect
for(k in 1:nyear){
# occupancy stand-year level effects
mu2ik[i,k] <- mu1i[i] + betaYr14*Year2014[k,i] + betaYr15*Year2015[k,i] + betaYr16*Year2016[k,i] +
betaYr17*Year2017[k, i] + betaYr18*Year2018[k, i] + betaYr19*Year2019[k, i] +
betaPre*PreTrt2[k,i] + betaPost*PostTrt2[k,i]
}
}
for(i in 1:nall){
# occupancy plot-level effects
mu3ikj[i] <- mu2ik[Stand3[i], Year3[i]] + betaDW*DW3[i]
log(lambda[i]) <- mu3ikj[i]
N[i] ~ dpois(lambda[i])
# detection model
logit(theta[i]) <- a0[Year3[i]] + aTrt[Year3[i]]*Trt3[i] + aAT[Year3[i]]*AT3[i] + aAT2[Year3[i]]*AT3[i]*AT3[i] +
aTrtAT[Year3[i]]*Trt3[i]*AT3[i] + aTrtAT2[Year3[i]]*Trt3[i]*AT3[i]*AT3[i]
innerterm[i] <- 1 - theta[i]
p[i] <- 1 - pow(innerterm[i], N[i])
for(j in 1:nvisit){
# likelihood
y[i,j] ~ dbin(p[i], 1)
}
}
}
sets <- expand.grid(R=c(20, 30, 40, 50, 60), S=c(5, 7, 9), posttrt=c(0.1, 0.3), det=c(0.15, 0.30, 0.50))
View(sets)
library(R2jags)
library(plyr)
# hierarchical occupancy model
model.h <- function(){
# priors
aInt.mean ~ dnorm(0,0.25)
aInt.sd ~ dgamma(2, 1)
aInt.tau <- 1/(aInt.sd * aInt.sd)
bTrt ~ dnorm(0, 0.25)
bYr ~ dnorm(0, 0.25)
bTrtYr ~ dnorm(0, 0.25)
gInt ~ dnorm(0, 0.333)
# stand-level model
for(i in 1:R){
b0[i] ~ dnorm(aInt.mean, aInt.tau)
}
for(i in 1:N){
bMean[i] <- b0[StandID[i]] + bTrt*Trt[i] + bYr*Yr[i] + bTrtYr*TrtYr[i]
}
# plot level model
for(j in 1:n){
logit(psi[j]) <- bMean[StandYr[j]]
Z[j] ~ dbin(psi[j], 1)
for(k in 1:3){
logit(p[j,k]) <- gInt
p.eff[j,k] <- Z[j]*p[j,k]
y[j,k] ~ dbin(p.eff[j,k], 1)
}
}
}
genData1 <- function(R, S, posttrt, det){
preTrtOcc <- 0.70
beta0 <- log(preTrtOcc/(1-preTrtOcc))
beta3 <- log((posttrt/(1-posttrt))/(preTrtOcc/(1-preTrtOcc)))
Dat <- expand.grid(Yr=c(0,1), Stand=1:R)
Dat$Trt <- ifelse(Dat$Stand <= R/2, 0, 1)
Dat$StandYr <- 1:nrow(Dat)
b0 <- rnorm(R, mean=0, sd=1)
Dat$b0 <- b0[Dat$Stand]
Dat$logitpsi <- with(Dat, beta0 + b0 + beta3*Trt*Yr)
Dat$psi <- 1/(1+exp(-Dat$logitpsi))
Dat2 <- data.frame(Yr = rep(Dat$Yr, each=S),
Stand = rep(Dat$Stand, each=S),
Trt = rep(Dat$Trt, each=S),
StandYr = rep(Dat$StandYr, each=S),
psi = rep(Dat$psi, each=S))
occ <- rbinom(nrow(Dat2), 1, Dat2$psi)
detection <- cbind(rbinom(nrow(Dat2), 1, det*occ),
rbinom(nrow(Dat2), 1, det*occ),
rbinom(nrow(Dat2), 1, det*occ))
zst <- apply(detection, 1, max)
test.data <- list(y=detection, R=R, N=R*2, n=R*S*2, Trt=Dat$Trt, Yr=Dat$Yr, TrtYr= Dat$Trt * Dat$Yr,
StandID=Dat$Stand, StandYr = Dat2$StandYr)
list(test.data=test.data, zst=zst, Z=occ, psi=Dat2$psi)
}
fit1 <- function(dat, params, model, n.chains, n.thin, n.iter, n.burnin){
inits <- function(){list(Z=dat$zst)}
out <- jags(data=dat$test.data, inits=inits, parameters.to.save=params, model.file=model,
n.chains=n.chains, n.thin=n.thin, n.iter=n.iter, n.burnin=n.burnin)
out
}
runSim <- function(sets, nsim, params, model, n.chains, n.thin, n.iter, n.burnin, dfile){
nsets <- nrow(sets)
for(i in 1:nsets){
R <- sets$R[i]
S <- sets$S[i]
posttrt <- sets$posttrt[i]
det <- sets$det[i]
for(j in 1:nsim){
dat.ij <- genData1(R=R, S=S, posttrt=posttrt, det=det)
fm.ij <- fit1(dat.ij, params=params, model=model, n.chains=n.chains, n.thin=n.thin, n.iter=n.iter, n.burnin=n.burnin)
fname <- paste("Nstand.", R, "_Nsub.", S, "_Posttrt.", posttrt, "_det.", det, "_sim.", j, ".csv",sep="")
write.csv(fm.ij$BUGSoutput$summary, paste(dfile, fname, sep=""))
}
}
}
sets <- expand.grid(R=c(20, 30, 40, 50, 60), S=c(5, 7, 9), posttrt=c(0.1, 0.3), det=c(0.15, 0.30, 0.50))
params <- c("aInt.mean", "bTrt", "bYr", "bTrtYr", "gInt")
nsim=500
system.time(temp <- runSim(sets=sets, nsim=nsim, params=params, model=model.h, n.chains=3, n.thin=10, n.iter=10000, n.burnin=5000, dfile="C:/"))
dfile
View(sets)
sets <- expand.grid(R=c(20, 30, 40, 50, 60), S=c(5, 7, 9), posttrt=c(0.1, 0.3), det=c(0.15, 0.30, 0.50))
params <- c("aInt.mean", "bTrt", "bYr", "bTrtYr", "gInt")
nsim=500
system.time(temp <- runSim(sets=sets, nsim=nsim, params=params, model=model.h, n.chains=3, n.thin=10, n.iter=10000, n.burnin=5000,
dfile="C:\Users\jasmi\OneDrive\Documents\Academic\OSU\Git\oss-occu\2015\output"))
sets <- expand.grid(R=c(20, 30, 40, 50, 60), S=c(5, 7, 9), posttrt=c(0.1, 0.3), det=c(0.15, 0.30, 0.50))
params <- c("aInt.mean", "bTrt", "bYr", "bTrtYr", "gInt")
nsim=500
system.time(temp <- runSim(sets=sets, nsim=nsim, params=params, model=model.h, n.chains=3, n.thin=10, n.iter=10000, n.burnin=5000,
dfile="C:/Users/jasmi/OneDrive/Documents/Academic/OSU/Git/oss-occu/2015/output"))
sets <- expand.grid(R=c(20, 30, 40, 50, 60), S=c(5), posttrt=c(0.1, 0.3), det=c(0.15, 0.30, 0.50))
params <- c("aInt.mean", "bTrt", "bYr", "bTrtYr", "gInt")
nsim=1
system.time(temp <- runSim(sets=sets, nsim=nsim, params=params, model=model.h, n.chains=3, n.thin=10, n.iter=10000, n.burnin=5000,
dfile="C:/Users/jasmi/OneDrive/Documents/Academic/OSU/Git/oss-occu/2015/output"))
sets <- expand.grid(R=c(20), S=c(5), posttrt=c(0.1, 0.3), det=c(0.15, 0.30, 0.50))
params <- c("aInt.mean", "bTrt", "bYr", "bTrtYr", "gInt")
nsim=2
system.time(temp <- runSim(sets=sets, nsim=nsim, params=params, model=model.h, n.chains=3, n.thin=10, n.iter=10000, n.burnin=5000,
dfile="C:/Users/jasmi/OneDrive/Documents/Academic/OSU/Git/oss-occu/2015/output"))
osslist[["OWBLM1"]]
unlink("Academic/OSU/Git/multivariate-analysis/homework2_cache", recursive = TRUE)
install.packages("rmarkdown")
install.packages("rmarkdown")
statsals <- stat.desc(sals)
knitr::opts_chunk$set(echo = TRUE)
setwd("C:/Users/jasmi/OneDrive/Documents/Academic/OSU/Git/multivariate-analysis")
library(vegan)
library(pastecs)
library(corrplot)
library(ggplot2)
library(ggpubr)
data <- read.csv("habitat.occu.complete.csv",row.names=1)
source("biostats.R")
colnames(data)
drop <- c(2:5,7:9,11:13,22)
data_subset <- data[,-drop]
sals <- data_subset[,12:17]
env <- data_subset[,1:11]
head(sals)
head(env)
na_count <- colSums(is.na(data))
print(na_count)
spe_pres <- apply(sals > 0, 2, sum) #Number of occurrences
sort(spe_pres)
ac <- table(unlist(sals)) #Number of cases for each abundance class
sals_red <- drop.var(sals, min.po=5)
ac2 <- table(unlist(sals_red))
par(mfrow = c(1,2))
barplot(ac,
las = 1,
xlab = "Abundance class",
ylab = "Frequency",
col = gray(length(ac): 0/length(ac)),
ylim=c(0,5000)
)
barplot(ac2,
las = 1,
xlab = "Abundance class, reduced",
ylab = "Frequency",
col = gray(length(ac): 0/length(ac)),
ylim=c(0,1500)
)
sals <- sals_red
statsals <- stat.desc(sals)
View(statsals)
statsals$coef.var
env_cont <- env[,c("elev","temp","hum","soil_moist_avg","jul_date")]
env.log <- data.trans(env_cont, method="log", plot=F)
env.power <- data.trans(env_cont, method="power", exp=0.5, plot=F)
env.asin <- data.trans(env_cont, method="asin", plot=F)
env.scal <- decostand(env_cont, "max") #Standardization by max value of each column
env.relsp <- decostand(env_cont, "total", MARGIN=2) #Standardization by column totals
env.zscore <- decostand(env_cont, "standardize") #Z-scores the data in each column
env.rel <- decostand(env_cont, "total") #Standardization by max value of each site/row
env.norm <- decostand(env_cont, "normalize") #Give a length of 1 to each row vector (chord
#transformation), very useful for Euclidean distances (PCA, RDA) and can be used on
#log-transformed data
env.hel <- decostand(env_cont, "hellinger") #Square root of relative values per site, obtained
#by applying chord transformation to square root transformed data
env.chi <- decostand(env_cont, "chi.square") #Double standardization by columns and rows
env.wis <- wisconsin(env_cont) #Wisconsin standardization, values are ranged by column maxima
#and then by site totals
#Let's see what each of these transformations looks like for the "Max Depth (m)" variable:
par(mfrow = c(2,2))
boxplot(env_cont$elev,
env.power$elev,
env.log$elev,
las = 1,
main = "Simple Transformations",
names = c("raw data","sqrt","log"),
col = "#FF5050"
)
boxplot(env.scal$elev,
env.relsp$elev,
env.zscore$elev,
las = 1,
main = "Standardizations by Variable",
names = c("max","total","Z-score"),
col = "#9BE1AF"
)
boxplot(env.hel$elev,
env.rel$elev,
env.norm$elev,
las = 1,
main = "Standardizations by Sites",
names = c("Hellinger","total","norm"),
col = "#46AFAA"
)
boxplot(env.chi$elev,
env.wis$elev,
las = 1,
main = "Double Standardizations",
names = c("Chi-square","Wisconsin"),
col = "#FAD223"
)
env_cont <- env[,c("elev","temp","hum","soil_moist_avg","jul_date")]
env.log <- data.trans(env_cont, method="log", plot=F)
env.power <- data.trans(env_cont, method="power", exp=0.5, plot=F)
env.asin <- data.trans(env_cont, method="asin", plot=F)
env.scal <- decostand(env_cont, "max") #Standardization by max value of each column
env.relsp <- decostand(env_cont, "total", MARGIN=2) #Standardization by column totals
env.zscore <- decostand(env_cont, "standardize") #Z-scores the data in each column
env.rel <- decostand(env_cont, "total") #Standardization by max value of each site/row
env.norm <- decostand(env_cont, "normalize") #Give a length of 1 to each row vector (chord
#transformation), very useful for Euclidean distances (PCA, RDA) and can be used on
#log-transformed data
env.hel <- decostand(env_cont, "hellinger") #Square root of relative values per site, obtained
#by applying chord transformation to square root transformed data
env.chi <- decostand(env_cont, "chi.square") #Double standardization by columns and rows
env.wis <- wisconsin(env_cont) #Wisconsin standardization, values are ranged by column maxima
#and then by site totals
#Let's see what each of these transformations looks like for the "Max Depth (m)" variable:
par(mfrow = c(2,2))
boxplot(env_cont$elev,
env.power$elev,
env.log$elev,
las = 1,
main = "Simple Transformations",
names = c("raw data","sqrt","log"),
col = "#FF5050"
)
boxplot(env.scal$elev,
env.relsp$elev,
env.zscore$elev,
las = 1,
main = "Standardizations by Variable",
names = c("max","total","Z-score"),
col = "#9BE1AF"
)
boxplot(env.hel$elev,
env.rel$elev,
env.norm$elev,
las = 1,
main = "Standardizations by Sites",
names = c("Hellinger","total","norm"),
col = "#46AFAA"
)
boxplot(env.chi$elev,
env.wis$elev,
las = 1,
main = "Double Standardizations",
names = c("Chi-square","Wisconsin"),
col = "#FAD223"
)
env_cont <- env[,c("elev","temp","hum","soil_moist_avg","jul_date")]
env.log <- data.trans(env_cont, method="log", plot=F)
env.power <- data.trans(env_cont, method="power", exp=0.5, plot=F)
env.asin <- data.trans(env_cont, method="asin", plot=F)
env.scal <- decostand(env_cont, "max") #Standardization by max value of each column
env.relsp <- decostand(env_cont, "total", MARGIN=2) #Standardization by column totals
env.zscore <- decostand(env_cont, "standardize") #Z-scores the data in each column
env.rel <- decostand(env_cont, "total") #Standardization by max value of each site/row
env.norm <- decostand(env_cont, "normalize") #Give a length of 1 to each row vector (chord
#transformation), very useful for Euclidean distances (PCA, RDA) and can be used on
#log-transformed data
env.hel <- decostand(env_cont, "hellinger") #Square root of relative values per site, obtained
#by applying chord transformation to square root transformed data
env.chi <- decostand(env_cont, "chi.square") #Double standardization by columns and rows
env.wis <- wisconsin(env_cont) #Wisconsin standardization, values are ranged by column maxima
#and then by site totals
#Let's see what each of these transformations looks like for the "Max Depth (m)" variable:
par(mfrow = c(4,1))
boxplot(env_cont$elev,
env.power$elev,
env.log$elev,
las = 1,
main = "Simple Transformations",
names = c("raw data","sqrt","log"),
col = "#FF5050"
)
env_cont <- env[,c("elev","temp","hum","soil_moist_avg","jul_date")]
head(env_cont)
pairs(env_cont,
panel = panel.smooth,
main = "Bivariate Plots with Smooth Curves")
P.corr <- cor(env_cont, method = "pearson", use = "complete.obs")
round(P.corr, 2)
dev.off()
corrplot(P.corr,
type = "upper",
order = "hclust",
tl.col = "black",
tl.srt = 45)
p1 <- ggplot(env) + geom_boxplot(aes(x = trt, y = soil_moist_avg), fill = "#FF5050", alpha=0.8)
p2 <- ggplot(env) + geom_boxplot(aes(x = trt, y = temp), fill = "#FF5050", alpha=0.8)
p3 <- ggplot(env) + geom_boxplot(aes(x = trt, y = hum), fill = "#FF5050", alpha=0.8)
p4 <- ggplot(env) + geom_boxplot(aes(x = trt, y = canopy_cov), fill = "#FF5050", alpha=0.8)
p5 <- ggplot(env) + geom_boxplot(aes(x = trt, y = dwd_cov), fill = "#FF5050", alpha=0.8)
ggarrange(p1, p2, p3, p4, p5, ncol=2, nrow=3)
tinytex::tlmgr_update()
tinytex::tlmgr_update()
install.packages('tinytex')
install.packages('tinytex')
tinytex::install_tinytex()
update.packages(ask = FALSE)  # This updates all packages without asking
setwd("C:/Users/jasmi/OneDrive/Documents/Academic/OSU/Git/multivariate-analysis")
source("Biostats.R")
source("coldiss.R")
# Define matrix D
D <- matrix(c(5, 4, 1, 2), nrow = 2, byrow = TRUE)
# Calculate eigenvalues and eigenvectors
eigen_D <- eigen(D)
# Display eigenvalues
cat("Eigenvalues:\n")
print(eigen_D$values)
# Display eigenvectors
cat("Eigenvectors:\n")
print(eigen_D$vectors)
library(vegan)
library(nomclust)
library(BioStatR)
library(RColorBrewer)
setwd("C:/Users/jasmi/OneDrive/Documents/Academic/OSU/Git/multivariate-analysis")
source("Biostats.R")
source("coldiss.R")
dat <- read.csv("Harney_Fishes_2007.csv", row.names = 1)
sub_dat <- subset(dat, SMU=="Malheur")
#In this case, we will subset the data to include only the Malheur sites since the entire dataset is too
#large to effectively visualize.
#3) Omitting species with zero observations and sites without fish
spp_N <- colSums(sub_dat[,16:ncol(sub_dat)])
spp_0 <- subset(spp_N, spp_N == 0)
omit <- names(spp_0)
dat2 <- sub_dat[,!(colnames(sub_dat) %in% omit)]
dat3 <- dat2[rowSums(dat2[,16:ncol(dat2)]) >0, ]
#4) Dealing with missing data
dat3$Herbaceous[is.na(dat3$Herbaceous)] <- 0
dat3$Ann_Herb[is.na(dat3$Ann_Herb)] <- 0
dat3 <- dat3[complete.cases(dat3$SiteLength),]
dat_final <- dat3
#5) Splitting the data set into environmental variables and species abundances
fish <- dat_final[,16:ncol(dat_final)]
env <- dat_final[,1:15]
#6) Dropping rare species
fish_red <- drop.var(fish, min.fo=1)
#7) Standardizing species observed abundance by sampling effort
fish_dens <- fish_red
for(i in 1:nrow(fish_red)){
fish_dens[i,] <- fish_red[i,]/env$SiteLength[i]
}
#8) Selecting relevant environmental data without covarying factors
drop <- c("Latitude","Longitude","SiteLength","SiteWidth","SurfaceArea")
env <- env[,!(colnames(env) %in% drop)]
env_cont <- env[,!(colnames(env) %in% c("SMU","Pop","NLCD_Cat"))]
env <- env[,!(colnames(env) %in% c("Ave_Max_D","Ann_Herb"))]
env_cont <- env_cont[,!(colnames(env_cont) %in% c("Ave_Max_D","Ann_Herb"))]
#9) Checking for outliers (remember, we are ignoring them for now)
#10) Transforming and standardizing the data as needed
log_fish_abu <- log(fish_red + 1)
log_fish_dens <- log(fish_dens + 1)
env_std <- decostand(env_cont, "max")
fish_occ <- data.trans(fish_red, method="power", exp=0, plot=F)
fish.sim <- sm(fish_occ)
fish.jac <- vegdist(fish_occ, method="jaccard")
plot(fish.jac, fish.sim,
xlab="Jaccard's coefficient",
ylab="Simple Matching coefficient",
pch=21,
col="black")
abline(0, 1, col="darkgray")
fish.sor <- vegdist(fish_occ, method="bray")
plot(fish.jac, fish.sor,
xlab="Jaccard's coefficient",
ylab="Sorensen's coefficient",
pch=21,
col="black")
abline(0, 1, col="darkgray")
coldiss(fish.jac, nc=5, byrank=FALSE, diag=TRUE)
source("Biostats.R")
source("coldiss.R")
coldiss(fish.jac, nc=5, byrank=FALSE, diag=TRUE)
fish.bray <- vegdist(log_fish_dens, method="bray")
fish.cho <- decostand(fish_dens, "normalize")
fish.cho <- dist(fish.cho)
fish.hel <- decostand(fish_dens, "hellinger")
fish.hel <- dist(fish.hel)
plot(fish.cho, fish.hel,
xlab="Chord distance",
ylab="Hellinger distance",
pch=21,
col="black")
abline(0, 1, col="darkgray")
plot(fish.bray, fish.hel,
xlab="Bray-Curtis distance",
ylab="Hellinger distance",
pch=21,
col="black")
abline(0, 1, col="darkgray")
coldiss(fish.bray, nc=5, byrank=FALSE, diag=TRUE)
coldiss(fish.hel, nc=5, byrank=FALSE, diag=TRUE)
fish.chi <- vegdist(log_fish_abu, method="chi")
env.euc <- vegdist(env_std, metric="euclidean")
env.gower <- daisy(env_cont, metric="gower")
plot(env.gower, env.euc,
xlab="Gower dissimilarity",
ylab="Euclidean distance of standardized data",
pch=21,
col="black")
abline(0, 1, col="darkgray")
fish_pa.t <- t(fish_occ)
fish.t.jac <- vegdist(fish_pa.t, "jaccard")
coldiss(fish.t.jac, diag=TRUE)
install.packages(gclus)
install.packages("gclus")
coldiss(fish.t.jac, diag=TRUE)
coldiss(fish.bray, nc=5, byrank=FALSE, diag=TRUE)
coldiss(fish.hel, nc=5, byrank=FALSE, diag=TRUE)
fish_pa.t <- t(fish_occ)
fish.t.jac <- vegdist(fish_pa.t, "jaccard")
coldiss(fish.t.jac, diag=TRUE)
fish_dens.t <- t(log_fish_dens)
fish.pears <- cor(fish_dens)
fish.ken <- cor(fish_dens, method="kendall")
fish.t.chi <- decostand(fish_dens.t, "chi.square")
fish.t.chi2 <- dist(fish.t.chi) #Chi-square
coldiss(fish.pears, diag=TRUE)
coldiss(fish.ken, diag=TRUE)
coldiss(fish.t.chi2, diag=TRUE)
env.pearson <- cor(env_std)
env.o <- order.single(env.pearson)
pairs(env_std[, env.o],
lower.panel = panel.smooth,
upper.panel = panel.cor,
diag.panel = panel.hist,
main = "Pearson Correlation Matrix")
env.ken <- cor(env_std, method="kendall")
env.o <- order.single(env.ken)
pairs(env_std[, env.o],
lower.panel = panel.smooth,
upper.panel = panel.cor,
method = "kendall",
diag.panel = panel.hist,
main = "Kendall Correlation Matrix")
library(vegan)
library(cluster)
